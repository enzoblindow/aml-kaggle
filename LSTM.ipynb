{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(49)\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import codecs\n",
    "import random\n",
    "import keras\n",
    "import sys\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Dropout, Embedding\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###Read in train\n",
    "###APPEND LABELS TO TRAINING DATA \n",
    "\n",
    "df_train = pd.read_csv('data/train_data.csv')\n",
    "df_train.drop(['is_duplicate'], axis= 1, inplace = True)\n",
    "df_labels = pd.read_csv('data/train_labels.csv')\n",
    "df_train = df_train.merge(df_labels)\n",
    "\n",
    "###Read in test\n",
    "\n",
    "test = pd.read_csv('data/test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###Create train and Cros-validation sets\n",
    "#train, CV = train_test_split(df_train, train_size = 0.8, random_state = 49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Transform into series\n",
    "###Train\n",
    "train_qs_1 = pd.Series(df_train['question1']) \n",
    "train_qs_2 = pd.Series(df_train['question2']) \n",
    "labels = pd.Series(df_train['is_duplicate'])\n",
    "train_ids = pd.Series(df_train['id'])\n",
    "\n",
    "###Test\n",
    "test_qs_1 = pd.Series(test['question1']) \n",
    "test_qs_2 = pd.Series(test['question2']) \n",
    "test_ids = pd.Series(test['test_id']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create full lists for text processing:\n",
    "\n",
    "all_texts = train_qs_1.astype(str).tolist() + train_qs_2.astype(str).tolist() + test_qs_1.astype(str).tolist() + test_qs_1.astype(str).tolist()\n",
    "\n",
    "train_q1 = train_qs_1.astype(str).tolist()\n",
    "train_q2 = train_qs_2.astype(str).tolist()\n",
    "\n",
    "test_q1 = test_qs_1.astype(str).tolist()\n",
    "test_q2 = test_qs_2.astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Create word index from Glove\n",
    "embeddings_index = {}\n",
    "glove_path = '/Users/tom/Msc Data Science/Machine Learning/Assignments/Quora/Glove.6B/glove.6B.300d.txt'\n",
    "glove = codecs.open(glove_path, encoding='utf-8')\n",
    "\n",
    "for row in glove:\n",
    "    word_dims = row.split(' ')\n",
    "    index = word_dims[0]\n",
    "    dims = np.asarray(word_dims[1:], dtype='float32')\n",
    "    embeddings_index[index] = dims\n",
    "    \n",
    "glove.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Tokenize all uing Keras Tokenizer\n",
    "\n",
    "##Fit tokenizer:\n",
    "max_tok_words = 100000\n",
    "tokenizer = Tokenizer(num_words=max_tok_words)\n",
    "tokenizer.fit_on_texts(all_texts)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "#Create sequences to tokenize\n",
    "\n",
    "train_seq_1 = tokenizer.texts_to_sequences(train_q1)\n",
    "train_seq_2 = tokenizer.texts_to_sequences(train_q2)\n",
    "\n",
    "test_seq_1 = tokenizer.texts_to_sequences(test_q1)\n",
    "test_seq_2 = tokenizer.texts_to_sequences(test_q2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify appropriate padding length:\n",
    "full_seq = train_seq_1 + train_seq_2 + test_seq_1 + test_seq_2\n",
    "#99.5th percentile\n",
    "max_pad_len = int(np.percentile([len(x) for x in full_seq],99.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply padding:\n",
    "\n",
    "padded_train_1 = pad_sequences(train_seq_1, maxlen=max_pad_len)\n",
    "padded_train_2 = pad_sequences(train_seq_2, maxlen=max_pad_len)\n",
    "\n",
    "padded_test_1 = pad_sequences(test_seq_1, maxlen=max_pad_len)\n",
    "padded_test_2 = pad_sequences(test_seq_2, maxlen=max_pad_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create word embeddings\n",
    "\n",
    "index_length = len(word_index)\n",
    "embedding_matrix = np.zeros((index_length+1, 300))\n",
    "\n",
    "\n",
    "for w, i in word_index.items():\n",
    "    \n",
    "    #if i >= index_length:\n",
    "    if i > index_length:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(w)\n",
    "    \n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Optional: Augment data to contain Q2 vs Q1 swapped - effectively doubling training data\n",
    "\n",
    "# padded_train_1 = np.concatenate([padded_train_1,padded_train_2], axis = 0)\n",
    "# padded_train_2 = np.concatenate([padded_train_2,padded_train_1], axis = 0)\n",
    "# labels = np.concatenate([labels,labels], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Embedding layer for LSTM\n",
    "embedding_layer = Embedding(index_length+1,300,weights=[embedding_matrix],input_length=max_pad_len)\n",
    "                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "lstm_nodes = [200,300,400]\n",
    "dense_nodes = [100,200,300]\n",
    "\n",
    "lstm_drop = [0.1,0.15,0.2,0.25,0.3]\n",
    "dense_drop = [0.1,0.15,0.2,0.25,0.3]\n",
    "\n",
    "dense_activation = ['relu','sigmoid']\n",
    "\n",
    "lstm_bidirectional = [True,False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random initialization\n",
    "\n",
    "lstm_nodes_choice = 300 #random.choice(lstm_nodes)\n",
    "dense_nodes_choice = 200 #random.choice(dense_nodes)\n",
    "\n",
    "lstm_drop_choice = 0.2 #random.choice(lstm_drop)\n",
    "dense_drop_choice = 0.2 #random.choice(dense_drop)\n",
    "\n",
    "dense_activation_choice = 'relu' #random.choice(dense_activation)\n",
    "\n",
    "lstm_bidirectional_choice = False #random.choice(lstm_bidirectional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build LSTM layer (Bidirectional if picked)\n",
    "if lstm_bidirectional_choice:    \n",
    "    lstm_layer = Bidirectional(LSTM(lstm_nodes_choice, dropout=lstm_drop_choice, recurrent_dropout=lstm_drop_choice))\n",
    "else:\n",
    "    lstm_layer = LSTM(lstm_nodes_choice, dropout=lstm_drop_choice, recurrent_dropout=lstm_drop_choice)\n",
    "\n",
    "#Question 1 input layer\n",
    "input_1 = Input(shape=(max_pad_len,), dtype='int32')\n",
    "embedded_1 = embedding_layer(input_1)\n",
    "q1 = lstm_layer(embedded_1)\n",
    "\n",
    "#Question 2 input layer\n",
    "input_2 = Input(shape=(max_pad_len,), dtype='int32')\n",
    "embedded_2 = embedding_layer(input_2)\n",
    "q2 = lstm_layer(embedded_2)\n",
    "\n",
    "#Combine outputs from q1 and q2\n",
    "combined_layer = concatenate([q1, q2])\n",
    "combined_layer = Dropout(lstm_drop_choice)(combined_layer)\n",
    "combined_layer = BatchNormalization()(combined_layer)\n",
    "\n",
    "#First Dense layer\n",
    "combined_layer = Dense(dense_nodes_choice, activation=dense_activation_choice)(combined_layer)\n",
    "combined_layer = Dropout(dense_drop_choice)(combined_layer)\n",
    "combined_layer = BatchNormalization()(combined_layer)\n",
    "\n",
    "#Second Dense layer\n",
    "combined_layer = Dense(dense_nodes_choice, activation=dense_activation_choice)(combined_layer)\n",
    "combined_layer = Dropout(dense_drop_choice)(combined_layer)\n",
    "combined_layer = BatchNormalization()(combined_layer)\n",
    "\n",
    "#Prediction Dense Layer\n",
    "prediction_layer = Dense(1, activation='sigmoid')(combined_layer)\n",
    "\n",
    "#Compile Model\n",
    "model = Model(inputs=[input_1, input_2],outputs=prediction_layer)\n",
    "model.compile(loss='binary_crossentropy',optimizer='nadam',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 258531 samples, validate on 64633 samples\n",
      "Epoch 1/5\n",
      "258531/258531 [==============================] - 2645s - loss: 0.5068 - acc: 0.7521 - val_loss: 0.4632 - val_acc: 0.7801\n",
      "Epoch 2/5\n",
      "147000/258531 [================>.............] - ETA: 1008s - loss: 0.4192 - acc: 0.8024"
     ]
    }
   ],
   "source": [
    "#Fit Model\n",
    "epochs = 5\n",
    "logging = model.fit([padded_train_1,padded_train_2], labels , validation_split = 0.2, epochs=epochs, batch_size=1000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_lstm = [lstm_nodes_choice] * epochs\n",
    "nodes_dense = [lstm_nodes_choice] * epochs\n",
    "\n",
    "drop_lstm = [lstm_drop_choice] * epochs\n",
    "drop_dense = [dense_drop_choice] * epochs\n",
    "\n",
    "activation_dense = [dense_activation_choice] * epochs\n",
    "\n",
    "lstm_bidirectional = [lstm_bidirectional_choice] * epochs\n",
    "\n",
    "epoch_cnt = range(1,epochs+1)\n",
    "\n",
    "acc = logging.history['acc']\n",
    "val_acc = logging.history['val_acc']\n",
    "loss = logging.history['loss']\n",
    "val_loss = logging.history['val_loss']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>activation_dense</th>\n",
       "      <th>drop_dense</th>\n",
       "      <th>drop_lstm</th>\n",
       "      <th>epoch_cnt</th>\n",
       "      <th>loss</th>\n",
       "      <th>lstm_bidirectional</th>\n",
       "      <th>nodes_dense</th>\n",
       "      <th>nodes_lstm</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.505632</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.894114</td>\n",
       "      <td>True</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.729662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.560701</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.806054</td>\n",
       "      <td>True</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.724996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.635795</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.682506</td>\n",
       "      <td>True</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.725618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.634543</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.686852</td>\n",
       "      <td>True</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.724493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.703379</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.597633</td>\n",
       "      <td>True</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.731577</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        acc activation_dense  drop_dense  drop_lstm  epoch_cnt      loss  \\\n",
       "0  0.505632          sigmoid        0.25        0.3          1  0.894114   \n",
       "1  0.560701          sigmoid        0.25        0.3          2  0.806054   \n",
       "2  0.635795          sigmoid        0.25        0.3          3  0.682506   \n",
       "3  0.634543          sigmoid        0.25        0.3          4  0.686852   \n",
       "4  0.703379          sigmoid        0.25        0.3          5  0.597633   \n",
       "\n",
       "   lstm_bidirectional  nodes_dense  nodes_lstm  val_acc  val_loss  \n",
       "0                True          250         250    0.365  0.729662  \n",
       "1                True          250         250    0.365  0.724996  \n",
       "2                True          250         250    0.365  0.725618  \n",
       "3                True          250         250    0.365  0.724493  \n",
       "4                True          250         250    0.365  0.731577  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance = pd.DataFrame(\n",
    "    {'nodes_lstm': nodes_lstm,\n",
    "    'nodes_dense': nodes_dense,\n",
    "    'drop_lstm': drop_lstm,\n",
    "    'drop_dense': drop_dense,\n",
    "    'activation_dense': activation_dense,\n",
    "    'lstm_bidirectional': lstm_bidirectional,\n",
    "    'epoch_cnt': epoch_cnt,\n",
    "    'acc': acc,\n",
    "    'val_acc': val_acc,\n",
    "    'loss': loss,\n",
    "    'val_loss': val_loss\n",
    "    })\n",
    "\n",
    "performance.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

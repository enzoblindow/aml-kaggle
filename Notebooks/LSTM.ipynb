{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(49)\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import codecs\n",
    "import random\n",
    "import keras\n",
    "import sys\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Dropout, Embedding\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###Read in train\n",
    "###APPEND LABELS TO TRAINING DATA \n",
    "\n",
    "df_train = pd.read_csv('data/train_data.csv')\n",
    "df_train.drop(['is_duplicate'], axis= 1, inplace = True)\n",
    "df_labels = pd.read_csv('data/train_labels.csv')\n",
    "df_train = df_train.merge(df_labels)\n",
    "\n",
    "###Read in test\n",
    "\n",
    "test = pd.read_csv('data/test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###Create train and Cros-validation sets\n",
    "#train, CV = train_test_split(df_train, train_size = 0.8, random_state = 49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Transform into series\n",
    "###Train\n",
    "train_qs_1 = pd.Series(df_train['question1']) \n",
    "train_qs_2 = pd.Series(df_train['question2']) \n",
    "labels = pd.Series(df_train['is_duplicate'])\n",
    "train_ids = pd.Series(df_train['id'])\n",
    "\n",
    "###Test\n",
    "test_qs_1 = pd.Series(test['question1']) \n",
    "test_qs_2 = pd.Series(test['question2']) \n",
    "test_ids = pd.Series(test['test_id']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create full lists for text processing:\n",
    "\n",
    "all_texts = train_qs_1.astype(str).tolist() + train_qs_2.astype(str).tolist() + test_qs_1.astype(str).tolist() + test_qs_1.astype(str).tolist()\n",
    "\n",
    "train_q1 = train_qs_1.astype(str).tolist()\n",
    "train_q2 = train_qs_2.astype(str).tolist()\n",
    "\n",
    "test_q1 = test_qs_1.astype(str).tolist()\n",
    "test_q2 = test_qs_2.astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Create word index from Glove\n",
    "embeddings_index = {}\n",
    "glove_path = '/Users/tom/Msc Data Science/Machine Learning/Assignments/Quora/Glove.6B/glove.6B.300d.txt'\n",
    "glove = codecs.open(glove_path, encoding='utf-8')\n",
    "\n",
    "for row in glove:\n",
    "    word_dims = row.split(' ')\n",
    "    index = word_dims[0]\n",
    "    dims = np.asarray(word_dims[1:], dtype='float32')\n",
    "    embeddings_index[index] = dims\n",
    "    \n",
    "glove.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Tokenize all uing Keras Tokenizer\n",
    "\n",
    "##Fit tokenizer:\n",
    "max_tok_words = 100000\n",
    "tokenizer = Tokenizer(num_words=max_tok_words)\n",
    "tokenizer.fit_on_texts(all_texts)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "#Create sequences to tokenize\n",
    "\n",
    "train_seq_1 = tokenizer.texts_to_sequences(train_q1)\n",
    "train_seq_2 = tokenizer.texts_to_sequences(train_q2)\n",
    "\n",
    "test_seq_1 = tokenizer.texts_to_sequences(test_q1)\n",
    "test_seq_2 = tokenizer.texts_to_sequences(test_q2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify appropriate padding length:\n",
    "full_seq = train_seq_1 + train_seq_2 + test_seq_1 + test_seq_2\n",
    "#99.5th percentile\n",
    "max_pad_len = int(np.percentile([len(x) for x in full_seq],99.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply padding:\n",
    "\n",
    "padded_train_1 = pad_sequences(train_seq_1, maxlen=max_pad_len)\n",
    "padded_train_2 = pad_sequences(train_seq_2, maxlen=max_pad_len)\n",
    "\n",
    "padded_test_1 = pad_sequences(test_seq_1, maxlen=max_pad_len)\n",
    "padded_test_2 = pad_sequences(test_seq_2, maxlen=max_pad_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create word embeddings\n",
    "\n",
    "index_length = len(word_index)\n",
    "embedding_matrix = np.zeros((index_length+1, 300))\n",
    "\n",
    "\n",
    "for w, i in word_index.items():\n",
    "    \n",
    "    #if i >= index_length:\n",
    "    if i > index_length:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(w)\n",
    "    \n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Optional: Augment data to contain Q2 vs Q1 swapped - effectively doubling training data\n",
    "\n",
    "# padded_train_1 = np.concatenate([padded_train_1,padded_train_2], axis = 0)\n",
    "# padded_train_2 = np.concatenate([padded_train_2,padded_train_1], axis = 0)\n",
    "# labels = np.concatenate([labels,labels], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Embedding layer for LSTM\n",
    "embedding_layer = Embedding(index_length+1,300,weights=[embedding_matrix],input_length=max_pad_len)\n",
    "                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "lstm_nodes = [200,300,400]\n",
    "dense_nodes = [100,200,300]\n",
    "\n",
    "lstm_drop = [0.1,0.15,0.2,0.25,0.3]\n",
    "dense_drop = [0.1,0.15,0.2,0.25,0.3]\n",
    "\n",
    "dense_activation = ['relu','sigmoid']\n",
    "\n",
    "lstm_bidirectional = [True,False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random initialization\n",
    "\n",
    "lstm_nodes_choice = 300 #random.choice(lstm_nodes)\n",
    "dense_nodes_choice = 200 #random.choice(dense_nodes)\n",
    "\n",
    "lstm_drop_choice = 0.2 #random.choice(lstm_drop)\n",
    "dense_drop_choice = 0.2 #random.choice(dense_drop)\n",
    "\n",
    "dense_activation_choice = 'relu' #random.choice(dense_activation)\n",
    "\n",
    "lstm_bidirectional_choice = False #random.choice(lstm_bidirectional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build LSTM layer (Bidirectional if picked)\n",
    "if lstm_bidirectional_choice:    \n",
    "    lstm_layer = Bidirectional(LSTM(lstm_nodes_choice, dropout=lstm_drop_choice, recurrent_dropout=lstm_drop_choice))\n",
    "else:\n",
    "    lstm_layer = LSTM(lstm_nodes_choice, dropout=lstm_drop_choice, recurrent_dropout=lstm_drop_choice)\n",
    "\n",
    "#Question 1 input layer\n",
    "input_1 = Input(shape=(max_pad_len,), dtype='int32')\n",
    "embedded_1 = embedding_layer(input_1)\n",
    "q1 = lstm_layer(embedded_1)\n",
    "\n",
    "#Question 2 input layer\n",
    "input_2 = Input(shape=(max_pad_len,), dtype='int32')\n",
    "embedded_2 = embedding_layer(input_2)\n",
    "q2 = lstm_layer(embedded_2)\n",
    "\n",
    "#Combine outputs from q1 and q2\n",
    "combined_layer = concatenate([q1, q2])\n",
    "combined_layer = Dropout(lstm_drop_choice)(combined_layer)\n",
    "combined_layer = BatchNormalization()(combined_layer)\n",
    "\n",
    "#First Dense layer\n",
    "combined_layer = Dense(dense_nodes_choice, activation=dense_activation_choice)(combined_layer)\n",
    "combined_layer = Dropout(dense_drop_choice)(combined_layer)\n",
    "combined_layer = BatchNormalization()(combined_layer)\n",
    "\n",
    "#Second Dense layer\n",
    "combined_layer = Dense(dense_nodes_choice, activation=dense_activation_choice)(combined_layer)\n",
    "combined_layer = Dropout(dense_drop_choice)(combined_layer)\n",
    "combined_layer = BatchNormalization()(combined_layer)\n",
    "\n",
    "#Prediction Dense Layer\n",
    "prediction_layer = Dense(1, activation='sigmoid')(combined_layer)\n",
    "\n",
    "#Compile Model\n",
    "model = Model(inputs=[input_1, input_2],outputs=prediction_layer)\n",
    "model.compile(loss='binary_crossentropy',optimizer='nadam',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 258531 samples, validate on 64633 samples\n",
      "Epoch 1/20\n",
      "258531/258531 [==============================] - 2592s - loss: 0.3834 - acc: 0.8219 - val_loss: 0.4750 - val_acc: 0.7626\n",
      "Epoch 2/20\n",
      "258531/258531 [==============================] - 2605s - loss: 0.2754 - acc: 0.8765 - val_loss: 0.5002 - val_acc: 0.8053\n",
      "Epoch 3/20\n",
      "258531/258531 [==============================] - 2463s - loss: 0.2361 - acc: 0.8961 - val_loss: 0.5256 - val_acc: 0.8068\n",
      "Epoch 4/20\n",
      "258531/258531 [==============================] - 2536s - loss: 0.2091 - acc: 0.9097 - val_loss: 0.5856 - val_acc: 0.8125\n",
      "Epoch 5/20\n",
      "258531/258531 [==============================] - 2153s - loss: 0.1867 - acc: 0.9195 - val_loss: 0.6136 - val_acc: 0.8027\n",
      "Epoch 6/20\n",
      "258531/258531 [==============================] - 2172s - loss: 0.1701 - acc: 0.9272 - val_loss: 0.6511 - val_acc: 0.8116\n",
      "Epoch 7/20\n",
      "258531/258531 [==============================] - 2634s - loss: 0.1525 - acc: 0.9359 - val_loss: 0.7197 - val_acc: 0.8162\n",
      "Epoch 8/20\n",
      "258531/258531 [==============================] - 2399s - loss: 0.1405 - acc: 0.9414 - val_loss: 0.7014 - val_acc: 0.8181\n",
      "Epoch 9/20\n",
      "258531/258531 [==============================] - 2448s - loss: 0.1280 - acc: 0.9470 - val_loss: 0.7349 - val_acc: 0.8120\n",
      "Epoch 10/20\n",
      "258531/258531 [==============================] - 2260s - loss: 0.1166 - acc: 0.9522 - val_loss: 0.7800 - val_acc: 0.8106\n",
      "Epoch 11/20\n",
      "258531/258531 [==============================] - 2171s - loss: 0.1084 - acc: 0.9558 - val_loss: 0.8068 - val_acc: 0.8139\n",
      "Epoch 12/20\n",
      "258531/258531 [==============================] - 2164s - loss: 0.0993 - acc: 0.9595 - val_loss: 0.8599 - val_acc: 0.8098\n",
      "Epoch 13/20\n",
      "258531/258531 [==============================] - 2147s - loss: 0.0934 - acc: 0.9622 - val_loss: 0.8392 - val_acc: 0.8112\n",
      "Epoch 14/20\n",
      "258531/258531 [==============================] - 2157s - loss: 0.0871 - acc: 0.9650 - val_loss: 0.9008 - val_acc: 0.8172\n",
      "Epoch 15/20\n",
      "258531/258531 [==============================] - 2153s - loss: 0.0815 - acc: 0.9673 - val_loss: 0.9122 - val_acc: 0.8165\n",
      "Epoch 16/20\n",
      "258531/258531 [==============================] - 2153s - loss: 0.0761 - acc: 0.9696 - val_loss: 0.9029 - val_acc: 0.8143\n",
      "Epoch 17/20\n",
      "258531/258531 [==============================] - 2150s - loss: 0.0719 - acc: 0.9715 - val_loss: 0.9298 - val_acc: 0.8148\n",
      "Epoch 18/20\n",
      "258531/258531 [==============================] - 2148s - loss: 0.0683 - acc: 0.9732 - val_loss: 0.9473 - val_acc: 0.8160\n",
      "Epoch 19/20\n",
      "258531/258531 [==============================] - 2168s - loss: 0.0640 - acc: 0.9749 - val_loss: 0.9238 - val_acc: 0.8143\n",
      "Epoch 20/20\n",
      "258531/258531 [==============================] - 2157s - loss: 0.0604 - acc: 0.9766 - val_loss: 0.9522 - val_acc: 0.8173\n"
     ]
    }
   ],
   "source": [
    "#Fit Model\n",
    "epochs = 20\n",
    "logging = model.fit([padded_train_1,padded_train_2], labels , validation_split = 0.2, epochs=epochs, batch_size=1000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_lstm = [lstm_nodes_choice] * epochs\n",
    "nodes_dense = [dense_nodes_choice] * epochs\n",
    "\n",
    "drop_lstm = [lstm_drop_choice] * epochs\n",
    "drop_dense = [dense_drop_choice] * epochs\n",
    "\n",
    "activation_dense = [dense_activation_choice] * epochs\n",
    "\n",
    "lstm_bidirectional = [lstm_bidirectional_choice] * epochs\n",
    "\n",
    "epoch_cnt = range(1,epochs+1)\n",
    "\n",
    "acc = logging.history['acc']\n",
    "val_acc = logging.history['val_acc']\n",
    "loss = logging.history['loss']\n",
    "val_loss = logging.history['val_loss']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.DataFrame(\n",
    "    {'nodes_lstm': nodes_lstm,\n",
    "    'nodes_dense': nodes_dense,\n",
    "    'drop_lstm': drop_lstm,\n",
    "    'drop_dense': drop_dense,\n",
    "    'activation_dense': activation_dense,\n",
    "    'lstm_bidirectional': lstm_bidirectional,\n",
    "    'epoch_cnt': epoch_cnt,\n",
    "    'acc': acc,\n",
    "    'val_acc': val_acc,\n",
    "    'loss': loss,\n",
    "    'val_loss': val_loss\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "master = master.append(performance,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master.to_csv(\"performance_out.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_predictions = model.predict([padded_test_1, padded_test_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame({\"test_id\":test_ids, \"nn_out\":test_predictions.ravel()})\n",
    "#test_df.to_csv(\"test_preds_for_logreg.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = model.predict([padded_train_1,padded_train_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame({\"id\":train_ids, \"nn_out\":train_predictions.ravel()})\n",
    "#train_df.to_csv(\"preds_for_logreg.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>activation_dense</th>\n",
       "      <th>drop_dense</th>\n",
       "      <th>drop_lstm</th>\n",
       "      <th>epoch_cnt</th>\n",
       "      <th>loss</th>\n",
       "      <th>lstm_bidirectional</th>\n",
       "      <th>nodes_dense</th>\n",
       "      <th>nodes_lstm</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.752150</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.506772</td>\n",
       "      <td>False</td>\n",
       "      <td>200</td>\n",
       "      <td>300</td>\n",
       "      <td>0.780097</td>\n",
       "      <td>0.463245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.803424</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.416057</td>\n",
       "      <td>False</td>\n",
       "      <td>200</td>\n",
       "      <td>300</td>\n",
       "      <td>0.796822</td>\n",
       "      <td>0.426125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.837749</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.353868</td>\n",
       "      <td>False</td>\n",
       "      <td>200</td>\n",
       "      <td>300</td>\n",
       "      <td>0.801556</td>\n",
       "      <td>0.423007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.865351</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.300552</td>\n",
       "      <td>False</td>\n",
       "      <td>200</td>\n",
       "      <td>300</td>\n",
       "      <td>0.806028</td>\n",
       "      <td>0.439879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.885840</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.259767</td>\n",
       "      <td>False</td>\n",
       "      <td>200</td>\n",
       "      <td>300</td>\n",
       "      <td>0.818081</td>\n",
       "      <td>0.456929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.823685</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.376889</td>\n",
       "      <td>False</td>\n",
       "      <td>300</td>\n",
       "      <td>400</td>\n",
       "      <td>0.765012</td>\n",
       "      <td>0.471562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.882355</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.264482</td>\n",
       "      <td>False</td>\n",
       "      <td>300</td>\n",
       "      <td>400</td>\n",
       "      <td>0.805053</td>\n",
       "      <td>0.465731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.900882</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.226010</td>\n",
       "      <td>False</td>\n",
       "      <td>300</td>\n",
       "      <td>400</td>\n",
       "      <td>0.813021</td>\n",
       "      <td>0.492255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.914088</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.199051</td>\n",
       "      <td>False</td>\n",
       "      <td>300</td>\n",
       "      <td>400</td>\n",
       "      <td>0.803939</td>\n",
       "      <td>0.571441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.924245</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.178023</td>\n",
       "      <td>False</td>\n",
       "      <td>300</td>\n",
       "      <td>400</td>\n",
       "      <td>0.818730</td>\n",
       "      <td>0.607322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.842851</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.339586</td>\n",
       "      <td>True</td>\n",
       "      <td>200</td>\n",
       "      <td>300</td>\n",
       "      <td>0.717234</td>\n",
       "      <td>0.565672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.904990</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.216202</td>\n",
       "      <td>True</td>\n",
       "      <td>200</td>\n",
       "      <td>300</td>\n",
       "      <td>0.798276</td>\n",
       "      <td>0.604551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.922884</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.181705</td>\n",
       "      <td>True</td>\n",
       "      <td>200</td>\n",
       "      <td>300</td>\n",
       "      <td>0.811273</td>\n",
       "      <td>0.587753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.931501</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.162071</td>\n",
       "      <td>True</td>\n",
       "      <td>200</td>\n",
       "      <td>300</td>\n",
       "      <td>0.803490</td>\n",
       "      <td>0.664947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.938247</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.147051</td>\n",
       "      <td>True</td>\n",
       "      <td>200</td>\n",
       "      <td>300</td>\n",
       "      <td>0.814661</td>\n",
       "      <td>0.680679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.717017</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.559748</td>\n",
       "      <td>False</td>\n",
       "      <td>200</td>\n",
       "      <td>300</td>\n",
       "      <td>0.739700</td>\n",
       "      <td>0.514301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.787051</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.448807</td>\n",
       "      <td>False</td>\n",
       "      <td>200</td>\n",
       "      <td>300</td>\n",
       "      <td>0.773924</td>\n",
       "      <td>0.468593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.828102</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.373272</td>\n",
       "      <td>False</td>\n",
       "      <td>200</td>\n",
       "      <td>300</td>\n",
       "      <td>0.802376</td>\n",
       "      <td>0.435111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.859506</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.313629</td>\n",
       "      <td>False</td>\n",
       "      <td>200</td>\n",
       "      <td>300</td>\n",
       "      <td>0.809741</td>\n",
       "      <td>0.432274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.880858</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.268299</td>\n",
       "      <td>False</td>\n",
       "      <td>200</td>\n",
       "      <td>300</td>\n",
       "      <td>0.809292</td>\n",
       "      <td>0.473865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.821917</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.383384</td>\n",
       "      <td>False</td>\n",
       "      <td>200</td>\n",
       "      <td>300</td>\n",
       "      <td>0.762644</td>\n",
       "      <td>0.474961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.876549</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.275390</td>\n",
       "      <td>False</td>\n",
       "      <td>200</td>\n",
       "      <td>300</td>\n",
       "      <td>0.805347</td>\n",
       "      <td>0.500219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.896136</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.236141</td>\n",
       "      <td>False</td>\n",
       "      <td>200</td>\n",
       "      <td>300</td>\n",
       "      <td>0.806817</td>\n",
       "      <td>0.525643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.909659</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.209065</td>\n",
       "      <td>False</td>\n",
       "      <td>200</td>\n",
       "      <td>300</td>\n",
       "      <td>0.812526</td>\n",
       "      <td>0.585559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.919545</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.186688</td>\n",
       "      <td>False</td>\n",
       "      <td>200</td>\n",
       "      <td>300</td>\n",
       "      <td>0.802717</td>\n",
       "      <td>0.613620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.927239</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.170080</td>\n",
       "      <td>False</td>\n",
       "      <td>200</td>\n",
       "      <td>300</td>\n",
       "      <td>0.811613</td>\n",
       "      <td>0.651087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.935865</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.152520</td>\n",
       "      <td>False</td>\n",
       "      <td>200</td>\n",
       "      <td>300</td>\n",
       "      <td>0.816162</td>\n",
       "      <td>0.719736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.941411</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.140466</td>\n",
       "      <td>False</td>\n",
       "      <td>200</td>\n",
       "      <td>300</td>\n",
       "      <td>0.818081</td>\n",
       "      <td>0.701421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.946977</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>9</td>\n",
       "      <td>0.128002</td>\n",
       "      <td>False</td>\n",
       "      <td>200</td>\n",
       "      <td>300</td>\n",
       "      <td>0.811985</td>\n",
       "      <td>0.734915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.952199</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.116562</td>\n",
       "      <td>False</td>\n",
       "      <td>200</td>\n",
       "      <td>300</td>\n",
       "      <td>0.810623</td>\n",
       "      <td>0.779960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.955839</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>11</td>\n",
       "      <td>0.108403</td>\n",
       "      <td>False</td>\n",
       "      <td>200</td>\n",
       "      <td>300</td>\n",
       "      <td>0.813857</td>\n",
       "      <td>0.806833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.959456</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>12</td>\n",
       "      <td>0.099344</td>\n",
       "      <td>False</td>\n",
       "      <td>200</td>\n",
       "      <td>300</td>\n",
       "      <td>0.809803</td>\n",
       "      <td>0.859853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.962217</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>13</td>\n",
       "      <td>0.093421</td>\n",
       "      <td>False</td>\n",
       "      <td>200</td>\n",
       "      <td>300</td>\n",
       "      <td>0.811242</td>\n",
       "      <td>0.839224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.964995</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>14</td>\n",
       "      <td>0.087099</td>\n",
       "      <td>False</td>\n",
       "      <td>200</td>\n",
       "      <td>300</td>\n",
       "      <td>0.817183</td>\n",
       "      <td>0.900825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.967323</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>15</td>\n",
       "      <td>0.081485</td>\n",
       "      <td>False</td>\n",
       "      <td>200</td>\n",
       "      <td>300</td>\n",
       "      <td>0.816518</td>\n",
       "      <td>0.912244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.969586</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>16</td>\n",
       "      <td>0.076097</td>\n",
       "      <td>False</td>\n",
       "      <td>200</td>\n",
       "      <td>300</td>\n",
       "      <td>0.814336</td>\n",
       "      <td>0.902915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.971497</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>17</td>\n",
       "      <td>0.071896</td>\n",
       "      <td>False</td>\n",
       "      <td>200</td>\n",
       "      <td>300</td>\n",
       "      <td>0.814754</td>\n",
       "      <td>0.929816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.973183</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>18</td>\n",
       "      <td>0.068256</td>\n",
       "      <td>False</td>\n",
       "      <td>200</td>\n",
       "      <td>300</td>\n",
       "      <td>0.815976</td>\n",
       "      <td>0.947303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.974939</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>19</td>\n",
       "      <td>0.064014</td>\n",
       "      <td>False</td>\n",
       "      <td>200</td>\n",
       "      <td>300</td>\n",
       "      <td>0.814290</td>\n",
       "      <td>0.923789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.976649</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.060439</td>\n",
       "      <td>False</td>\n",
       "      <td>200</td>\n",
       "      <td>300</td>\n",
       "      <td>0.817291</td>\n",
       "      <td>0.952163</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         acc activation_dense  drop_dense  drop_lstm  epoch_cnt      loss  \\\n",
       "0   0.752150             relu         0.2        0.2          1  0.506772   \n",
       "1   0.803424             relu         0.2        0.2          2  0.416057   \n",
       "2   0.837749             relu         0.2        0.2          3  0.353868   \n",
       "3   0.865351             relu         0.2        0.2          4  0.300552   \n",
       "4   0.885840             relu         0.2        0.2          5  0.259767   \n",
       "5   0.823685             relu         0.2        0.2          1  0.376889   \n",
       "6   0.882355             relu         0.2        0.2          2  0.264482   \n",
       "7   0.900882             relu         0.2        0.2          3  0.226010   \n",
       "8   0.914088             relu         0.2        0.2          4  0.199051   \n",
       "9   0.924245             relu         0.2        0.2          5  0.178023   \n",
       "10  0.842851             relu         0.2        0.2          1  0.339586   \n",
       "11  0.904990             relu         0.2        0.2          2  0.216202   \n",
       "12  0.922884             relu         0.2        0.2          3  0.181705   \n",
       "13  0.931501             relu         0.2        0.2          4  0.162071   \n",
       "14  0.938247             relu         0.2        0.2          5  0.147051   \n",
       "15  0.717017          sigmoid         0.2        0.2          1  0.559748   \n",
       "16  0.787051          sigmoid         0.2        0.2          2  0.448807   \n",
       "17  0.828102          sigmoid         0.2        0.2          3  0.373272   \n",
       "18  0.859506          sigmoid         0.2        0.2          4  0.313629   \n",
       "19  0.880858          sigmoid         0.2        0.2          5  0.268299   \n",
       "20  0.821917             relu         0.2        0.2          1  0.383384   \n",
       "21  0.876549             relu         0.2        0.2          2  0.275390   \n",
       "22  0.896136             relu         0.2        0.2          3  0.236141   \n",
       "23  0.909659             relu         0.2        0.2          4  0.209065   \n",
       "24  0.919545             relu         0.2        0.2          5  0.186688   \n",
       "25  0.927239             relu         0.2        0.2          6  0.170080   \n",
       "26  0.935865             relu         0.2        0.2          7  0.152520   \n",
       "27  0.941411             relu         0.2        0.2          8  0.140466   \n",
       "28  0.946977             relu         0.2        0.2          9  0.128002   \n",
       "29  0.952199             relu         0.2        0.2         10  0.116562   \n",
       "30  0.955839             relu         0.2        0.2         11  0.108403   \n",
       "31  0.959456             relu         0.2        0.2         12  0.099344   \n",
       "32  0.962217             relu         0.2        0.2         13  0.093421   \n",
       "33  0.964995             relu         0.2        0.2         14  0.087099   \n",
       "34  0.967323             relu         0.2        0.2         15  0.081485   \n",
       "35  0.969586             relu         0.2        0.2         16  0.076097   \n",
       "36  0.971497             relu         0.2        0.2         17  0.071896   \n",
       "37  0.973183             relu         0.2        0.2         18  0.068256   \n",
       "38  0.974939             relu         0.2        0.2         19  0.064014   \n",
       "39  0.976649             relu         0.2        0.2         20  0.060439   \n",
       "\n",
       "    lstm_bidirectional  nodes_dense  nodes_lstm   val_acc  val_loss  \n",
       "0                False          200         300  0.780097  0.463245  \n",
       "1                False          200         300  0.796822  0.426125  \n",
       "2                False          200         300  0.801556  0.423007  \n",
       "3                False          200         300  0.806028  0.439879  \n",
       "4                False          200         300  0.818081  0.456929  \n",
       "5                False          300         400  0.765012  0.471562  \n",
       "6                False          300         400  0.805053  0.465731  \n",
       "7                False          300         400  0.813021  0.492255  \n",
       "8                False          300         400  0.803939  0.571441  \n",
       "9                False          300         400  0.818730  0.607322  \n",
       "10                True          200         300  0.717234  0.565672  \n",
       "11                True          200         300  0.798276  0.604551  \n",
       "12                True          200         300  0.811273  0.587753  \n",
       "13                True          200         300  0.803490  0.664947  \n",
       "14                True          200         300  0.814661  0.680679  \n",
       "15               False          200         300  0.739700  0.514301  \n",
       "16               False          200         300  0.773924  0.468593  \n",
       "17               False          200         300  0.802376  0.435111  \n",
       "18               False          200         300  0.809741  0.432274  \n",
       "19               False          200         300  0.809292  0.473865  \n",
       "20               False          200         300  0.762644  0.474961  \n",
       "21               False          200         300  0.805347  0.500219  \n",
       "22               False          200         300  0.806817  0.525643  \n",
       "23               False          200         300  0.812526  0.585559  \n",
       "24               False          200         300  0.802717  0.613620  \n",
       "25               False          200         300  0.811613  0.651087  \n",
       "26               False          200         300  0.816162  0.719736  \n",
       "27               False          200         300  0.818081  0.701421  \n",
       "28               False          200         300  0.811985  0.734915  \n",
       "29               False          200         300  0.810623  0.779960  \n",
       "30               False          200         300  0.813857  0.806833  \n",
       "31               False          200         300  0.809803  0.859853  \n",
       "32               False          200         300  0.811242  0.839224  \n",
       "33               False          200         300  0.817183  0.900825  \n",
       "34               False          200         300  0.816518  0.912244  \n",
       "35               False          200         300  0.814336  0.902915  \n",
       "36               False          200         300  0.814754  0.929816  \n",
       "37               False          200         300  0.815976  0.947303  \n",
       "38               False          200         300  0.814290  0.923789  \n",
       "39               False          200         300  0.817291  0.952163  "
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import save_model\n",
    "save_model(model, 'models/lstm-final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "from helpers import save_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# init logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# init tqdm\n",
    "try:\n",
    "    if get_ipython().__class__.__name__ == 'ZMQInteractiveShell':\n",
    "        tqdm_notebook().pandas()\n",
    "    else:\n",
    "        tqdm.pandas()\n",
    "except NameError:\n",
    "    tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the dataframes\n",
    "Define functions to enable easy building of dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_dfs = [\n",
    "    {\n",
    "        'df': '../data/{}_features.csv',\n",
    "        'cols': [\n",
    "            'last_char',\n",
    "             'avg_shared_words',\n",
    "             'word_count_diff',\n",
    "             'levenshtein',\n",
    "             'shared_words_pcnt',\n",
    "             'avg_shared_trigrams',\n",
    "             'shared_bigram_pcnt',\n",
    "             'shared_trigram_pcnt',\n",
    "             'avg_shared_quadgrams',\n",
    "             'shared_quadgram_pcnt',\n",
    "             'shared_entities',\n",
    "             'non_shared_entities',\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'df': '../data/{}_lstm_output.csv',\n",
    "        'cols': [\n",
    "            'nn_out'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'df': '../data/tfidf_{}_features.csv',\n",
    "        'cols': [\n",
    "            'tfidf_word_match_share'\n",
    "        ]\n",
    "    },\n",
    "     {\n",
    "         'df': '../data/topic_modelling_output.csv',\n",
    "         'cols': []\n",
    "     },\n",
    "    {\n",
    "        'df': '../data/{}_with_sim_and_ents_long.csv',\n",
    "        'cols': [\n",
    "            'CARDINAL_1','DATE_1','EVENT_1','FAC_1','GPE_1','LANGUAGE_1','LAW_1','LOC_1','MONEY_1','NORP_1',\n",
    "            'ORDINAL_1','ORG_1','PERCENT_1','PERSON_1','PRODUCT_1','QUANTITY_1','TIME_1','WORK_OF_ART_1',\n",
    "            'CARDINAL_2','DATE_2','EVENT_2','FAC_2','GPE_2','LANGUAGE_2','LAW_2','LOC_2','MONEY_2','NORP_2',\n",
    "            'ORDINAL_2','ORG_2','PERCENT_2','PERSON_2','PRODUCT_2','QUANTITY_2','TIME_2','WORK_OF_ART_2',\n",
    "        ]\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_df(df, merge_col='id', _set='train'):\n",
    "    \"\"\"\n",
    "    Creates the dataframe for either train or test set identically.\n",
    "    \n",
    "    Parameter\n",
    "        df: base dataframe containing the ids and questions\n",
    "        merge_col: specify the column name how to merge the dataframe together\n",
    "        _set: pass either 'test' or 'train'\n",
    "        \n",
    "    Returns\n",
    "        df: fully merged dataframe\n",
    "    \"\"\"\n",
    "    for _df in _dfs:\n",
    "        path = _df['df'].format(_set)\n",
    "        df = df.merge(pd.read_csv(path).loc[:,[merge_col] + _df['cols']], on=merge_col, how='left')\n",
    "        logging.info('Merged in {}'.format(path))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_results_set(df, preds_array, file_path):\n",
    "    \"\"\"\n",
    "    Builds the csv in the format that can be uploaded to kaggle.\n",
    "    \n",
    "    Parameter\n",
    "        df: test dataframe containing the test ids and that was used to make the predictions\n",
    "        preds_array: the predicition array that was return by the model\n",
    "        file_path: specify the path and file name to store the output csv\n",
    "    \"\"\"\n",
    "    p = pd.DataFrame({\"test_id\": df['test_id']})\n",
    "    p['is_duplicate'] = preds_array\n",
    "    p['is_duplicate'] = np.around(p['is_duplicate'].values)\n",
    "    p.is_duplicate = p.is_duplicate.astype(int)\n",
    "    p.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../data/train_data.csv')\n",
    "df_train = df_train.drop(['is_duplicate'], axis=1).merge(pd.read_csv('../data/train_labels.csv'), on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = build_df(df_train, merge_col='id', _set='train')\n",
    "df_train = df_train.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('../data/test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = build_df(df_test, merge_col='test_id', _set='test')\n",
    "df_test = df_test.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "Classify test data using logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>last_char</th>\n",
       "      <th>avg_shared_words</th>\n",
       "      <th>word_count_diff</th>\n",
       "      <th>levenshtein</th>\n",
       "      <th>shared_words_pcnt</th>\n",
       "      <th>avg_shared_trigrams</th>\n",
       "      <th>shared_bigram_pcnt</th>\n",
       "      <th>shared_trigram_pcnt</th>\n",
       "      <th>avg_shared_quadgrams</th>\n",
       "      <th>shared_quadgram_pcnt</th>\n",
       "      <th>...</th>\n",
       "      <th>MONEY_2</th>\n",
       "      <th>NORP_2</th>\n",
       "      <th>ORDINAL_2</th>\n",
       "      <th>ORG_2</th>\n",
       "      <th>PERCENT_2</th>\n",
       "      <th>PERSON_2</th>\n",
       "      <th>PRODUCT_2</th>\n",
       "      <th>QUANTITY_2</th>\n",
       "      <th>TIME_2</th>\n",
       "      <th>WORK_OF_ART_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.647482</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.069565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.365217</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   last_char  avg_shared_words  word_count_diff  levenshtein  \\\n",
       "0        1.0              12.0              2.0     0.926829   \n",
       "1        1.0               4.0              5.0     0.647482   \n",
       "2        1.0               4.0              4.0     0.454545   \n",
       "3        1.0               0.0              2.0     0.069565   \n",
       "4        1.0               2.0              6.0     0.365217   \n",
       "\n",
       "   shared_words_pcnt  avg_shared_trigrams  shared_bigram_pcnt  \\\n",
       "0           0.923077                  9.0            0.833333   \n",
       "1           0.380952                  0.0            0.105263   \n",
       "2           0.333333                  0.0            0.090909   \n",
       "3           0.000000                  0.0            0.000000   \n",
       "4           0.200000                  0.0            0.000000   \n",
       "\n",
       "   shared_trigram_pcnt  avg_shared_quadgrams  shared_quadgram_pcnt  \\\n",
       "0             0.818182                   8.0                   0.8   \n",
       "1             0.000000                   0.0                   0.0   \n",
       "2             0.000000                   0.0                   0.0   \n",
       "3             0.000000                   0.0                   0.0   \n",
       "4             0.000000                   0.0                   0.0   \n",
       "\n",
       "       ...        MONEY_2  NORP_2  ORDINAL_2  ORG_2  PERCENT_2  PERSON_2  \\\n",
       "0      ...            0.0     0.0        0.0    0.0        0.0       0.0   \n",
       "1      ...            0.0     1.0        0.0    0.0        0.0       2.0   \n",
       "2      ...            0.0     0.0        0.0    1.0        0.0       0.0   \n",
       "3      ...            0.0     0.0        0.0    0.0        0.0       0.0   \n",
       "4      ...            0.0     0.0        0.0    0.0        0.0       0.0   \n",
       "\n",
       "   PRODUCT_2  QUANTITY_2  TIME_2  WORK_OF_ART_2  \n",
       "0        0.0         0.0     0.0            0.0  \n",
       "1        0.0         0.0     0.0            0.0  \n",
       "2        0.0         0.0     0.0            0.0  \n",
       "3        0.0         0.0     0.0            0.0  \n",
       "4        0.0         0.0     0.0            0.0  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df_train['is_duplicate']\n",
    "x = df_train.drop(['id', 'question1', 'question2', 'is_duplicate'], axis=1)\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.150956\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:           is_duplicate   No. Observations:               323164\n",
      "Model:                          Logit   Df Residuals:                   323114\n",
      "Method:                           MLE   Df Model:                           49\n",
      "Date:                Sun, 03 Dec 2017   Pseudo R-squ.:                  0.7707\n",
      "Time:                        14:36:28   Log-Likelihood:                -48784.\n",
      "converged:                       True   LL-Null:                   -2.1275e+05\n",
      "                                        LLR p-value:                     0.000\n",
      "==========================================================================================\n",
      "                             coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------------\n",
      "last_char                 -3.4974      0.036    -95.851      0.000      -3.569      -3.426\n",
      "avg_shared_words          -0.0619      0.008     -7.445      0.000      -0.078      -0.046\n",
      "word_count_diff           -0.0969      0.003    -34.081      0.000      -0.103      -0.091\n",
      "levenshtein               -2.5782      0.085    -30.302      0.000      -2.745      -2.411\n",
      "shared_words_pcnt          2.8579      0.142     20.143      0.000       2.580       3.136\n",
      "avg_shared_trigrams        0.6832      0.041     16.861      0.000       0.604       0.763\n",
      "shared_bigram_pcnt         0.3208      0.150      2.135      0.033       0.026       0.615\n",
      "shared_trigram_pcnt       -4.0304      0.288    -14.019      0.000      -4.594      -3.467\n",
      "avg_shared_quadgrams      -0.5560      0.039    -14.281      0.000      -0.632      -0.480\n",
      "shared_quadgram_pcnt       1.1683      0.239      4.878      0.000       0.699       1.638\n",
      "shared_entities            0.2866      0.040      7.163      0.000       0.208       0.365\n",
      "non_shared_entities       -0.1751      0.021     -8.318      0.000      -0.216      -0.134\n",
      "nn_out                     6.7326      0.022    302.708      0.000       6.689       6.776\n",
      "tfidf_word_match_share     0.9388      0.040     23.362      0.000       0.860       1.018\n",
      "CARDINAL_1                 0.0095      0.039      0.245      0.806      -0.066       0.085\n",
      "DATE_1                    -0.0397      0.041     -0.958      0.338      -0.121       0.041\n",
      "EVENT_1                    0.0443      0.125      0.356      0.722      -0.200       0.288\n",
      "FAC_1                      0.0728      0.175      0.416      0.678      -0.270       0.416\n",
      "GPE_1                     -0.1189      0.033     -3.619      0.000      -0.183      -0.055\n",
      "LANGUAGE_1                -0.1081      0.109     -0.993      0.321      -0.321       0.105\n",
      "LAW_1                      0.0548      0.227      0.242      0.809      -0.389       0.499\n",
      "LOC_1                      0.1966      0.080      2.472      0.013       0.041       0.353\n",
      "MONEY_1                   -0.3489      0.148     -2.356      0.018      -0.639      -0.059\n",
      "NORP_1                    -0.0920      0.047     -1.960      0.050      -0.184   -7.94e-06\n",
      "ORDINAL_1                 -0.1857      0.095     -1.956      0.050      -0.372       0.000\n",
      "ORG_1                     -0.2194      0.029     -7.565      0.000      -0.276      -0.163\n",
      "PERCENT_1                 -0.2406      0.227     -1.062      0.288      -0.685       0.204\n",
      "PERSON_1                  -0.0569      0.037     -1.538      0.124      -0.129       0.016\n",
      "PRODUCT_1                 -0.1075      0.082     -1.314      0.189      -0.268       0.053\n",
      "QUANTITY_1                 0.0100      0.099      0.101      0.920      -0.184       0.204\n",
      "TIME_1                    -0.2219      0.125     -1.779      0.075      -0.467       0.023\n",
      "WORK_OF_ART_1             -0.1728      0.091     -1.901      0.057      -0.351       0.005\n",
      "CARDINAL_2                -0.0250      0.037     -0.667      0.505      -0.098       0.048\n",
      "DATE_2                    -0.1193      0.041     -2.946      0.003      -0.199      -0.040\n",
      "EVENT_2                    0.1124      0.124      0.908      0.364      -0.130       0.355\n",
      "FAC_2                     -0.3231      0.183     -1.763      0.078      -0.682       0.036\n",
      "GPE_2                     -0.1535      0.032     -4.742      0.000      -0.217      -0.090\n",
      "LANGUAGE_2                 0.0357      0.110      0.324      0.746      -0.180       0.252\n",
      "LAW_2                     -0.0728      0.236     -0.308      0.758      -0.535       0.390\n",
      "LOC_2                     -0.2050      0.079     -2.606      0.009      -0.359      -0.051\n",
      "MONEY_2                   -0.1366      0.156     -0.877      0.381      -0.442       0.169\n",
      "NORP_2                    -0.1394      0.047     -2.938      0.003      -0.232      -0.046\n",
      "ORDINAL_2                 -0.2497      0.093     -2.679      0.007      -0.432      -0.067\n",
      "ORG_2                     -0.1556      0.028     -5.501      0.000      -0.211      -0.100\n",
      "PERCENT_2                 -0.1363      0.193     -0.707      0.480      -0.514       0.242\n",
      "PERSON_2                  -0.0810      0.037     -2.167      0.030      -0.154      -0.008\n",
      "PRODUCT_2                 -0.1847      0.082     -2.256      0.024      -0.345      -0.024\n",
      "QUANTITY_2                 0.0003      0.104      0.003      0.997      -0.204       0.205\n",
      "TIME_2                    -0.0951      0.125     -0.758      0.448      -0.341       0.151\n",
      "WORK_OF_ART_2             -0.2954      0.090     -3.280      0.001      -0.472      -0.119\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "lm = sm.Logit(y, x)\n",
    "result = lm.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (323164,50) and (81126,100) not aligned: 50 (dim 1) != 81126 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-bcc02c09bf79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'question1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'question2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/laurentiusblindow/.virtualenvs/aml-kaggle/lib/python2.7/site-packages/statsmodels/discrete/discrete_model.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, params, exog, linear)\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0mexog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (323164,50) and (81126,100) not aligned: 50 (dim 1) != 81126 (dim 0)"
     ]
    }
   ],
   "source": [
    "lm.predict(df_test.drop(['test_id', 'question1', 'question2'], axis=1).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network\n",
    "Classify test data using neural network\n",
    "\n",
    "Built in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nfeatures = 50\n",
    "\n",
    "y = df_train['is_duplicate'].values\n",
    "X = df_train.drop(['id', 'question1', 'question2', 'is_duplicate'], axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn = models.Sequential()\n",
    "nn.add(layers.Dense(units=200, activation='relu', input_shape=(nfeatures,)))\n",
    "nn.add(layers.Dense(units=50, activation='relu'))\n",
    "nn.add(layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hist = nn.fit(X, y, epochs=100, verbose=1, batch_size=1000, validation_split = 1/3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = nn.predict_classes(df_test.drop(['test_id', 'question1', 'question2'], axis=1).values)\n",
    "build_results_set(df_test, y_pred, 'nn_binary_classification3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_model(network, '../models/nn-classification-200-50-dense/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM\n",
    "Clasify test data using SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = df_train['is_duplicate']\n",
    "X = df_train.drop(['id', 'question1', 'question2', 'is_duplicate'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sv = svm.SVC(kernel='linear', C=1, gamma=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sv.fit(X_train.values, y_train.values)\n",
    "sv.score(X_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = sv.predict(X_test.values)\n",
    "print(r2_score(y_test.values, y_pred))\n",
    "print(mean_squared_error(y_test.values, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = sv.predict(df_test.drop(['test_id', 'question1', 'question2'], axis=1).values)\n",
    "build_results_set(df_test, y_pred, 'svm_classification1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "Classify test data using random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = df_train['is_duplicate']\n",
    "X = df_train.drop(['id', 'question1', 'question2', 'is_duplicate'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.00734771  0.07335273  0.30867292  0.06791558  0.02375899\n",
      "  0.07877609  0.00463782  0.02962118  0.00840603  0.00072106  0.03134265\n",
      "  0.17746575  0.18340498  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.0042285\n",
      "  0.          0.          0.          0.          0.          0.00034801\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "forest.fit(X_train.values, y_train.values)\n",
    "print(forest.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.648709281351\n",
      "0.0818510009846\n"
     ]
    }
   ],
   "source": [
    "y_pred = forest.predict(X_test.values)\n",
    "print(r2_score(y_test.values, y_pred))\n",
    "print(mean_squared_error(y_test.values, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = forest.predict(df_test.drop(['test_id', 'question1', 'question2'], axis=1).values)\n",
    "build_results_set(df_test, y_pred, 'randomforest_classification1.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

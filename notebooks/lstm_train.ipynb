{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model\n",
    "##### Built in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(49)\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import codecs\n",
    "import logging\n",
    "import pickle\n",
    "import random\n",
    "import keras\n",
    "import sys\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers.merge import concatenate, subtract\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Dropout, Embedding\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from helpers import save_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in training data and append training labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(train_data_path='../data/train_data.csv', train_labels_path='../data/train_data.csv'):\n",
    "    df_train = pd.read_csv(train_data_path)\n",
    "    df_train.drop(['is_duplicate'], axis= 1, inplace = True)\n",
    "    df_labels = pd.read_csv(train_labels_path)\n",
    "    logging.info('loaded training data')\n",
    "    return df_train.merge(df_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(test_data_path='../data/test_data.csv'):\n",
    "    logging.info('loaded test data')\n",
    "    return pd.read_csv(test_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform train and test sets into series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = get_train_data()\n",
    "df_test = get_test_data()\n",
    "\n",
    "train_qs_1 = pd.Series(df_train['question1']) \n",
    "train_qs_2 = pd.Series(df_train['question2'])\n",
    "labels = pd.Series(df_train['is_duplicate'])\n",
    "\n",
    "test_qs_1 = pd.Series(df_test['question1']) \n",
    "test_qs_2 = pd.Series(df_test['question2']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create full text lists for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = train_qs_1.astype(str).tolist() + train_qs_2.astype(str).tolist() + test_qs_1.astype(str).tolist() + test_qs_1.astype(str).tolist()\n",
    "\n",
    "train_q1 = train_qs_1.astype(str).tolist()\n",
    "train_q2 = train_qs_2.astype(str).tolist()\n",
    "\n",
    "test_q1 = test_qs_1.astype(str).tolist()\n",
    "test_q2 = test_qs_2.astype(str).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Keras Tokenizer, fit to all questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOK_WORDS = 100000\n",
    "tokenizer = Tokenizer(num_words=MAX_TOK_WORDS)\n",
    "tokenizer.fit_on_texts(all_texts)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform questions to word sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sequence(text_series, tokenizer):\n",
    "    return tokenizer.texts_to_sequences(text_series.astype(str).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq_1 = make_sequence(train_qs_1, tokenizer)\n",
    "train_seq_2 = make_sequence(train_qs_2, tokenizer)\n",
    "\n",
    "test_seq_1 = make_sequence(test_qs_1, tokenizer)\n",
    "test_seq_2 = make_sequence(test_qs_2, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify appropriate padding length\n",
    "Take 99.5th percentile of sequence lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_seq = train_seq_1 + train_seq_2 + test_seq_1 + test_seq_2\n",
    "\n",
    "max_pad_len = int(np.percentile([len(x) for x in full_seq], 99.5))  # 36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply padding to sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_train_1 = pad_sequences(train_seq_1, maxlen=max_pad_len)\n",
    "padded_train_2 = pad_sequences(train_seq_2, maxlen=max_pad_len)\n",
    "\n",
    "padded_test_1 = pad_sequences(test_seq_1, maxlen=max_pad_len)\n",
    "padded_test_2 = pad_sequences(test_seq_2, maxlen=max_pad_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate class weights due to unbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_weight = float(df_labels['is_duplicate'].sum())/df_labels['is_duplicate'].count()\n",
    "non_weight = 1 - dup_weight\n",
    "\n",
    "re_weight = non_weight/dup_weight\n",
    "\n",
    "class_weight = {0 : 1., 1: re_weight}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create word index from Glove (Glove file stored locally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(glove_path):\n",
    "    logging.info('loading embeddings from gloVe file')\n",
    "    embeddings_index = {}\n",
    "    try:\n",
    "        glove = codecs.open(glove_path, encoding='utf-8')\n",
    "    except IOError:\n",
    "        logging.warning('no glove embeddings file supplied. please visit http://nlp.stanford.edu/data/glove.6B.zip and copy the file glove.6B.300d.txt into this directory')\n",
    "\n",
    "    for row in glove:\n",
    "        word_dims = row.split(' ')\n",
    "        index = word_dims[0]\n",
    "        dims = np.asarray(word_dims[1:], dtype='float32')\n",
    "        embeddings_index[index] = dims\n",
    "\n",
    "    glove.close()\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(word_index, glove_path=os.getcwd()+'/glove.6B.300d.txt', save=True):\n",
    "    logging.info('creating word embeddings')\n",
    "    embeddings_index = load_embeddings(glove_path)\n",
    "    index_length = len(word_index)\n",
    "    embedding_matrix = np.zeros((index_length+1, 300))\n",
    "\n",
    "    for w, i in word_index.items():\n",
    "        if i > index_length:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(w)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    if save:\n",
    "        logging.info('saving embeddings to file')\n",
    "        with open('../models/embedding_matrix.txt', 'wb') as filepath:\n",
    "            pickle.dump(embedding_matrix, filepath)\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(filename='../models/embedding_matrix.txt'):\n",
    "    if os.path.isfile(filename):\n",
    "        logging.info('loading embeddings from file')\n",
    "        with open(filename, 'rb') as filepath:\n",
    "                return pickle.load(filepath)\n",
    "    else:\n",
    "        logging.warning('No embeddings found, please create embeddings for the data provided in the /notebooks/lstm_train notebook.')\n",
    "        return create_embeddings(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create embedding layer for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index)+1, 300, weights=[embedding_matrix], input_length=max_pad_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define possible values for hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_nodes = [200,300,400]\n",
    "dense_nodes = [100,200,300]\n",
    "\n",
    "lstm_drop = [0.1,0.15,0.2,0.25,0.3]\n",
    "dense_drop = [0.1,0.15,0.2,0.25,0.3]\n",
    "\n",
    "dense_activation = ['relu','sigmoid']\n",
    "\n",
    "lstm_bidirectional = [True,False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize hyperparameters by randomly selecting from defined options\n",
    "\n",
    "This method is used to test performance of hyperparameters and can be overridden manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_nodes_choice = random.choice(lstm_nodes)\n",
    "dense_nodes_choice = random.choice(dense_nodes)\n",
    "\n",
    "lstm_drop_choice = random.choice(lstm_drop)\n",
    "dense_drop_choice = random.choice(dense_drop)\n",
    "\n",
    "dense_activation_choice = random.choice(dense_activation)\n",
    "\n",
    "lstm_bidirectional_choice = random.choice(lstm_bidirectional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Builds LSTM layer (bidirectional LSTM layer if chosen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lstm_bidirectional_choice:    \n",
    "    lstm_layer = Bidirectional(LSTM(lstm_nodes_choice,\n",
    "                                     dropout=lstm_drop_choice,\n",
    "                                     recurrent_dropout=lstm_drop_choice\n",
    "                                   ))\n",
    "else:\n",
    "    lstm_layer = LSTM(lstm_nodes_choice,\n",
    "                       dropout=lstm_drop_choice,\n",
    "                       recurrent_dropout=lstm_drop_choice\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input layers for Question 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1 = Input(shape=(max_pad_len,), dtype='int32')\n",
    "embedded_1 = embedding_layer(input_1)\n",
    "q1 = lstm_layer(embedded_1)\n",
    "\n",
    "input_2 = Input(shape=(max_pad_len,), dtype='int32')\n",
    "embedded_2 = embedding_layer(input_2)\n",
    "q2 = lstm_layer(embedded_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the outputs of the Q1 and Q2 LSTM layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_layer = subtract([q1, q2])\n",
    "combined_layer = Dropout(lstm_drop_choice)(combined_layer)\n",
    "combined_layer = BatchNormalization()(combined_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_layer = Dense(dense_nodes_choice, activation=dense_activation_choice)(combined_layer)\n",
    "combined_layer = Dropout(dense_drop_choice)(combined_layer)\n",
    "combined_layer = BatchNormalization()(combined_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second Dense layer (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_layer = Dense(dense_nodes_choice, activation=dense_activation_choice)(combined_layer)\n",
    "# combined_layer = Dropout(dense_drop_choice)(combined_layer)\n",
    "# combined_layer = BatchNormalization()(combined_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction (output) layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_layer = Dense(1, activation='sigmoid')(combined_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[input_1, input_2],outputs=prediction_layer)\n",
    "model.compile(loss='binary_crossentropy',optimizer='nadam',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "logging = model.fit([padded_train_1,padded_train_2], labels , validation_split = 0.2, \n",
    "                    epochs=epochs, batch_size=1000, shuffle=True, class_weight = class_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '..')\n",
    "save_model(model, '../models/model_1/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use model to predict test data and export as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict([padded_test_1, padded_test_2])\n",
    "test_df = pd.DataFrame({\"test_id\":pd.Series(df_test['test_id']), \"nn_out\":test_predictions.ravel()})\n",
    "test_df.to_csv(\"../data/test_lstm_output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use model to predict train data and export as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = model.predict([padded_train_1,padded_train_2])\n",
    "train_df = pd.DataFrame({\"id\":pd.Series(df_train['id']), \"nn_out\":train_predictions.ravel()})\n",
    "train_df.to_csv(\"../data/train_lstm_output.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

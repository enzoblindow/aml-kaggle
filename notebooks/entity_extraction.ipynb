{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "from helpers import get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e93b3f38bf84e4a988d1511284c521f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style=u'info', max=1), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# init logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# init tqdm\n",
    "try:\n",
    "    if get_ipython().__class__.__name__ == 'ZMQInteractiveShell':\n",
    "        tqdm_notebook().pandas()\n",
    "    else:\n",
    "        tqdm.pandas()\n",
    "except NameError:\n",
    "    tqdm.pandas()\n",
    "\n",
    "# init spaCy\n",
    "nlp = spacy.load('en')  # english corpus\n",
    "nlp = spacy.load('en_core_web_lg')  # english word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'Apple', 0, 5, u'ORG')\n",
      "(u'U.K.', 27, 31, u'GPE')\n",
      "(u'$1 billion', 44, 54, u'MONEY')\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "# nlp = spacy.load('en')\n",
    "# doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "\n",
    "# for ent in doc.ents:\n",
    "#     print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add entities as a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_entities(str):\n",
    "    entities = {}\n",
    "    if isinstance(str, unicode):\n",
    "        for token in nlp(unicode(str)).ents:\n",
    "            if not token.label_ in entities:\n",
    "                entities[token.label_] = 1\n",
    "            else:\n",
    "                entities[token.label_] += 1\n",
    "        if len(entities) > 0:\n",
    "            return entities\n",
    "        else:\n",
    "            return np.nan\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'dog', True, 7.0336733, False)\n",
      "(u'cat', True, 6.6808186, False)\n",
      "(u'banana', True, 6.7000141, False)\n",
      "(u'sasquatch', True, 6.9789977, False)\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "# nlp = spacy.load('en_core_web_lg')\n",
    "# tokens = nlp(u'dog cat banana sasquatch')\n",
    "\n",
    "# for token in tokens:\n",
    "#     print(token.text, token.has_vector, token.vector_norm, token.is_oov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add similarity as a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_similarity(str1, str2):\n",
    "    if isinstance(str1, unicode) and isinstance(str2, unicode):\n",
    "        return nlp(str1).similarity(nlp(str2))\n",
    "    else:\n",
    "        np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply entity extraction and similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dummify entities from stored dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_entitity_column(df, counter):\n",
    "    logging.info(df.columns)\n",
    "    for col in df.columns:\n",
    "        if col in ENTS:\n",
    "            new = '{}_{}'.format(col, counter)\n",
    "            logging.info('renaming {} to {}'.format(col, new))\n",
    "            df.rename(columns={'{}'.format(col): new}, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def string_to_dict(dict_string):\n",
    "    # Convert to proper json format\n",
    "    dict_string = dict_string.replace(\"'\", '\"').replace('u\"', '\"')\n",
    "    return json.loads(dict_string)\n",
    "\n",
    "def get_features(dataframe):\n",
    "    # get all unique entities\n",
    "    dataframe.entities1 = dataframe.entities1.progress_apply(lambda x: string_to_dict(x) if isinstance(x, str) else np.nan)\n",
    "    dataframe = pd.concat([dataframe, dataframe['entities1'].progress_apply(pd.Series).fillna(0)], axis=1)\n",
    "    rename_entitity_column(dataframe, 1)\n",
    "    dataframe.entities2 = dataframe.entities2.progress_apply(lambda x: string_to_dict(x) if isinstance(x, str) else np.nan)\n",
    "    dataframe = pd.concat([dataframe, dataframe['entities2'].progress_apply(pd.Series).fillna(0)], axis=1)\n",
    "    rename_entitity_column(dataframe, 2)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "process application to data in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCHSIZE = 1000\n",
    "df = get_data(unicoded=True)\n",
    "\n",
    "counter = 1\n",
    "for batch in range(0, len(df), BATCHSIZE):\n",
    "    end = batch+BATCHSIZE if batch+BATCHSIZE < len(df) else len(df)\n",
    "    logging.info('Starting batch {} from {} to {}'.format(counter, batch, end))\n",
    "    tdf = df.iloc[batch:batch+BATCHSIZE]\n",
    "    tdf.question1 = tdf.question1.apply(lambda x: unicoder(x))\n",
    "    tdf.question2 = tdf.question2.apply(lambda x: unicoder(x))\n",
    "    tdf['entities1'] = tdf.loc[:, 'question1'].progress_apply(lambda x: find_entities(x))\n",
    "    tdf['entities2'] = tdf.loc[:, 'question2'].progress_apply(lambda x: find_entities(x))\n",
    "    tdf['similarity_score'] = tdf.progress_apply(lambda row: apply_similarity(row['question1'], row['question2']), axis=1)\n",
    "    tdf.to_csv('batches/batch_{}.csv'.format(counter))\n",
    "    counter += 1\n",
    "logging.info('Finished {} batches'.format(counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking up all viable entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTS = {u'CARDINAL', u'DATE', u'EVENT', u'FAC', u'GPE', u'LANGUAGE', u'LAW', u'LOC', u'MONEY', u'NORP', u'ORDINAL',\n",
    "        u'ORG', u'PERCENT', u'PERSON', u'PRODUCT', u'QUANTITY', u'TIME', u'WORK_OF_ART'}\n",
    "\n",
    "# for row in train_df.loc[:, 'entities1']:\n",
    "#     if isinstance(row, dict):\n",
    "#         for k in row.keys():\n",
    "#             ENTS.add(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_data(unicoded=True)\n",
    "for filename in os.listdir('../batches'):\n",
    "    if filename.endswith('.csv'):\n",
    "        batch_df = pd.read_csv('../batches/{}'.format(filename))\n",
    "        df = train_df.merge(batch_df.loc[:, ['id', 'entities1', 'entities2', 'similarity_score']], on='id', how='left')\n",
    "        if filename != 'batch_1.csv':\n",
    "            df['entities1'] = df['entities1_y'].fillna(df['entities1_x'])\n",
    "            df['entities2'] = df['entities2_y'].fillna(df['entities2_x'])\n",
    "            df['similarity_score'] = df['similarity_score_y'].fillna(df['similarity_score_x'])\n",
    "            df.drop(['entities1_x', 'entities1_y'], axis=1, inplace=True)\n",
    "            df.drop(['entities2_x', 'entities2_y'], axis=1, inplace=True)\n",
    "            df.drop(['similarity_score_x', 'similarity_score_y'], axis=1, inplace=True)\n",
    "        logging.info('Merged in {}, new length {}'.format(filename, len(df.loc[df.similarity_score.isna() == False])))\n",
    "        continue\n",
    "    else:\n",
    "        continue\n",
    "final = get_features(df.loc[train_df.similarity_score.isna() == False])\n",
    "final.to_csv('data/train_with_sim_and_ents.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "apply also to test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('data/test_data.csv')\n",
    "test.question1 = test.question1.apply(lambda x: unicoder(x))\n",
    "test.question2 = test.question2.apply(lambda x: unicoder(x))\n",
    "test['entities1'] = test.loc[:, 'question1'].progress_apply(lambda x: find_entities(x))\n",
    "test['entities2'] = test.loc[:, 'question2'].progress_apply(lambda x: find_entities(x))\n",
    "test['similarity_score'] = test.progress_apply(lambda row: apply_similarity(row['question1'], row['question2']), axis=1)\n",
    "test.to_csv('data/test_with_sim_and_ents.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.entities1 = test.entities1.progress_apply(lambda x: string_to_dict(x) if isinstance(x, str) else np.nan)\n",
    "test = pd.concat([test, test['entities1'].apply(pd.Series).fillna(0)], axis=1)\n",
    "rename_entitity_column(test, 1)\n",
    "test.entities2 = test.entities2.progress_apply(lambda x: string_to_dict(x) if isinstance(x, str) else np.nan)\n",
    "test = pd.concat([test, test['entities2'].apply(pd.Series).fillna(0)], axis=1)\n",
    "rename_entitity_column(test, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml-kaggle",
   "language": "python",
   "name": "aml-kaggle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
